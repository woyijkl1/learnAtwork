### 数据离散化

方式：

将连续特征按照等宽（比如对于年龄 0-10 10-20 划分）或者等频率（分位数 比如 0 0.25 0.5 ）或者聚类划分成离散特征

好处：

- 对于异常值有鲁棒性，例如有一个年龄为300的用户属于异常值
- 防止过拟合，例如特征范围本来是0 -10 现在特征从0.2波动到0.3 其实没有明显的变化，但是模型还是需要去拟合这个变化，如果全是这种特征，模型过拟合
- 可以更加方便的进行特征交叉
- 对于某些特定的模型例如lr 或者tree based 模型，它们的假设就是数据服从某些空间的线性分布，离散化有利于将数据线性化
- 对于某些特定的模型，例如lr tree之类的，离散特征能大大减少模型的计算量或者存储量

坏处：

- 对于分类边界的特征抖动过大，比如分界点为0.5的二分类，然后0.499 和0.501属于完全不同的两个值，这个需要根据业务去合理划分

本质：数据离散化的本质实际上是用人来将数据抽象成特征，也可以理解为特征工程，例如给一个输入是0到2w连续范围，模型怎么能知道这个数字代表什么呢，如果二分离散化，模型就知道，哦 这某个特征，输入要么符合这个标准要么不符合这个标准。 深度学习中的激活函数也有这个功能，极端一点，激活函数用阶跃函数，就是将特征离散化到二值特征，神经网络里面也有离散化的作用，区别在于，神经网络能自动帮我们做离散化（但做的一定比人好吗？一定比人能提取更加服务业务的特征吗？ 不一定）

在神经网络里面的使用离散化可以结合embedding 然后将特征送入不同的层进行高度复杂的特征交叉，而离散化特征一般只能放在网络的输入再接几个mlp进行分类，特征交叉能力受限。当然也可以把离散特征加到网络的输入层，这个还没尝试过，也还没仔细想过。







### 数据归一化

方式：

归一化： （x - x_min)/(x_max-x_min) 

标准化：  （x-样本均值）/样本方差

两种方法实际上都是对数据进行线性变换，都是 （x-常量)/常量   的形式

但“归一化”的方式受极大 极小值的影响很大，但是其将数据转换到【0，1】区间，“标准化”的方式的常量受数据整体分布的影响，数据转化的范围不确定

总的来说推荐“标准化”，当数据范围一定要【0，1】的时候，需要用归一化



好处：

- 可以加速模型收敛

    理由，考虑房价预测问题 y = a x_1 + b x_2  其中 a b 为参数  x_1 x_2为房间数量和 房间面积

  ​             当x_1 x_2 未归一化的时候，损失函数 J = ( 3x_1 + 600x_2 - y_correct )^2 这个时候损失函数是一个椭圆

  ​             优化过程比较曲折

  ​			 当 x_1 x_2 归一化后，损失函数 J=（0.5x_1 + 0.55x_2 - y_correct）^2 这个时候损失函数是一个圆，优化过程比较直接（https://www.zhihu.com/question/20455227   忆臻的答案）







### 特征交叉

https://zhuanlan.zhihu.com/p/269730650

```
FM
```

<img src="/Users/rongdi.yin/Desktop/screenshot/截屏2021-07-15 下午5.35.48.png" alt="截屏2021-07-15 下午5.35.48" style="zoom:33%;" />

```
对于每个原始特征，FM都会学习一个隐向量。模型通过穷举所有的特征对，并进行逐一检测特征对的权值来自动识别出有效的特征组合。特征对的权值则是通过该特征对涉及的两个原始特征的隐向量的内积来计算。
两两交叉  形成二维交叉项， 用vec点成代替权重参数，防止参数爆炸

```





```
FNN
```

<img src="/Users/rongdi.yin/Desktop/screenshot/截屏2021-07-15 下午5.42.53.png" alt="截屏2021-07-15 下午5.42.53" style="zoom:33%;" />

```
增强FM模型的思路，就是用FM模型学习到的隐向量初始化深度神经网络模型（MLP），再由MLP完成最终学习（因为FM没有更高层的特征交叉）

x是输入的特征，它是大规模离散稀疏的。它可以分成N个Field，每一个Field中，只有一个值为1，其余都为0（即one-hot）

采用FM预训练得到的隐含层及其权重作为神经网络的第一层的初始值，之后再不断堆叠全连接层，最终输出预测的点击率.

可以将FNN理解成一种特殊的embedding+MLP，其要求第一层嵌入后的各领域特征维度一致，并且嵌入权重的初始化是FM预训练好的

```





```
PNN
```

<img src="/Users/rongdi.yin/Desktop/screenshot/截屏2021-07-15 下午5.49.30.png" alt="截屏2021-07-15 下午5.49.30" style="zoom:33%;" />

```
Z部分其实就是将Embedding层学到的嵌入直接原封不动地搬来
P部分：两两交叉 乘积

交叉的部分可以是： 向量内积（inner product）和外积（outer product）
```



```
deepFM
```

<img src="/Users/rongdi.yin/Desktop/screenshot/截屏2021-07-15 下午5.52.45.png" alt="截屏2021-07-15 下午5.52.45" style="zoom:33%;" />

```
FM + DNN

FM(是显示一阶+二阶向量级交叉)与DNN（隐式高阶bit级交叉）两部分的组合

FM layer可以抽取低纬特征，对稀疏输入非常友好，而深度神经网络可以获取高维特征，具有很好的对于隐含特征关系的挖掘能力
```





```
DCN
```

<img src="/Users/rongdi.yin/Library/Application Support/typora-user-images/image-20210716144144621.png" alt="image-20210716144144621" style="zoom:33%;" />

<img src="/Users/rongdi.yin/Library/Application Support/typora-user-images/image-20210716144213656.png" alt="image-20210716144213656" style="zoom:25%;" />

```
红色框的更新状态过程 遵循上面的公式。
有点类似rnn的感觉。
模型右边mlp学习高阶交叉特征，左边叉乘特征，左边增加参数的数量是 2*d*L_c (每层的权重可以共享)

```





```
xDeepFM

```

<img src="/Users/rongdi.yin/Library/Application Support/typora-user-images/image-20210716150637302.png" alt="image-20210716150637302" style="zoom:35%;" />



<img src="/Users/rongdi.yin/Library/Application Support/typora-user-images/image-20210716150756577.png" alt="image-20210716150756577" style="zoom:33%;" />

<img src="/Users/rongdi.yin/Library/Application Support/typora-user-images/image-20210716150732055.png" alt="image-20210716150732055" style="zoom:33%;" />

```
x^k 的更新过程：
x^0 每一行是一个特征的embedding
x^k的每一行： 把x^k-1层的对应行的元素与x^0的每一行两两相乘（对应位置元素相乘，结果还是向量），然后把这些向量加权求和。
```

```
CIN 保持了DCN的优点：有限高阶、自动叉乘、参数共享
而且是向量维度的叉乘，不是element wise的叉乘
```







```
autoInt
```

<img src="/Users/rongdi.yin/Library/Application Support/typora-user-images/image-20210716153951099.png" alt="image-20210716153951099" style="zoom:25%;" />

```
输入就是把transformer的step输入变成 特征vector，相当于每个特征是一个word embedding
用transformer进行特征交叉
```















