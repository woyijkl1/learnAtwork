{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this notebook is for nlp neural project framwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  the whole system consists of  5 block: \n",
    "1. tokenizer\n",
    "2. embedding\n",
    "3. dataset\n",
    "4. train_test\n",
    "5. arg_parser\n",
    "\n",
    "the whole process is followed by: parser->tokenizer->embedding->dataset->train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, max_seq_len=0, lower=True,pad_all=False):\n",
    "        self.lower = lower\n",
    "        self.pad_all = pad_all\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "\n",
    "    def fit_on_text(self, text):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.idx\n",
    "                self.idx2word[self.idx] = word\n",
    "                self.idx += 1\n",
    "                \n",
    "    def pad_and_truncate(self,sequence, maxlen, dtype='int64', padding='post',\n",
    "                         truncating='post', value=0, pad_all=False):\n",
    "        x = (np.ones(maxlen) * value).astype(dtype)\n",
    "        if truncating == 'pre':\n",
    "            trunc = sequence[-maxlen:]\n",
    "        else:\n",
    "            trunc = sequence[:maxlen]\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if padding == 'post':\n",
    "            x[:len(trunc)] = trunc\n",
    "        else:\n",
    "            x[-len(trunc):] = trunc\n",
    "        if pad_all:\n",
    "            return x\n",
    "        else:\n",
    "            return trunc\n",
    "    \n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        words = text.split()\n",
    "        unknownidx = len(self.word2idx)+1\n",
    "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        if self.max_seq_len !=0:\n",
    "            return self.pad_and_truncate(sequence, self.max_seq_len,padding=padding\n",
    "                                         ,truncating=truncating, pad_all = self.pad_all)\n",
    "        else:\n",
    "            return sequence\n",
    "\n",
    "def build_tokenizer(save_name, text=None，max_seq_len=0, pad_all=False):\n",
    "    if os.path.exists(save_fname):\n",
    "        print('loading tokenizer:', save_fname)\n",
    "        tokenizer = pickle.load(open(save_fname, 'rb'))\n",
    "    else:\n",
    "        print('building tokenizer:')\n",
    "        tokenizer = Tokenizer(max_seq_len)\n",
    "        tokenizer.fit_on_text(text)\n",
    "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
    "    return tokenizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_word_vec(path, word2idx=None):\n",
    "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    word_vec = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if word2idx is None or tokens[0] in word2idx.keys():\n",
    "            word_vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def build_embedding_matrix(word2idx, embed_dim, load_path, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print('loading embedding_matrix:', save_path)\n",
    "        embedding_matrix = pickle.load(open(save_path, 'rb'))\n",
    "    else:\n",
    "        print('loading word vectors...')\n",
    "        embedding_matrix = np.random.uniform(-1e-2,1e-2,size=(len(word2idx) + 2, embed_dim))\n",
    "        embedding_matrix[0] = 0\n",
    "        # embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
    "        fname = load_path\n",
    "        word_vec = _load_word_vec(fname, word2idx=word2idx)\n",
    "        print('building embedding_matrix:', save_path)\n",
    "        print('hit rate is: ',float(len(word_vec))/len(word2idx))\n",
    "        for word, i in word2idx.items():\n",
    "            vec = word_vec.get(word)\n",
    "            if vec is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = vec\n",
    "        pickle.dump(embedding_matrix, open(save_path, 'wb'))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_to_modify(data.Dataset):\n",
    "    def __init__(self,da,sort=False):\n",
    "        self.context = da[0]\n",
    "        self.label = da[1]\n",
    "        if sort:\n",
    "            self.sort_data()\n",
    "    def sort_data(self):\n",
    "        data_len = [len(data) for data in self.context]\n",
    "        sort_index = torch.sort(torch.tensor(data_len),descending=True)[1].long()\n",
    "        self.context = [self.context[i] for i in sort_index]\n",
    "        self.label = [self.label[i] for i in sort_index]\n",
    "    def __len__(self):\n",
    "        return len(self.context)\n",
    "    def __getitem__(self, index):\n",
    "        return self.context[index] , self.label[index]\n",
    "\n",
    "def collate_fn_no_shuffle(batch):\n",
    "    '''\n",
    "    :param batch: batch[0] is tensor by defalt\n",
    "    :return:\n",
    "    '''\n",
    "    if isinstance(batch[0],collections.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [collate_fn_rand(samples) for samples in transposed]\n",
    "    elif torch.is_tensor(batch[0]):\n",
    "        # 如果是标量，对应数据的label\n",
    "        if batch[0].shape == torch.Size([]):\n",
    "            return torch.stack(batch)\n",
    "\n",
    "        max_len = max([len(i) for i in batch])\n",
    "        pad_batch = [torch.tensor(i.numpy().tolist()+[0]*(max_len - len(i))).long()for i in batch]\n",
    "        pad_batch = torch.stack(pad_batch)\n",
    "        return pad_batch\n",
    "    elif isinstance(batch[0],int):\n",
    "        return torch.tensor(batch).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random select and pad\n",
    "mydata = myds(da)\n",
    "myloader = data.DataLoader(dataset=mydata,batch_size=3,shuffle=True,collate_fn=collate_fn_no_shuffle)\n",
    "for context ,label in myloader:\n",
    "    print(context,label)\n",
    "    print()\n",
    "\n",
    "# for less pad (shuffle in train function,after get context and label)\n",
    "import ipdb\n",
    "lp_dataset = myds(da,sort=True)\n",
    "lp_loader = data.DataLoader(dataset=mydata,batch_size=3,shuffle=False,collate_fn=collate_fn_no_shuffle)\n",
    "for context ,label in lp_loader:\n",
    "    print(context,label)\n",
    "    print('---------------')\n",
    "    \n",
    "    rand_id = torch.randperm(context.shape[0])\n",
    "    context_shuffle = context[rand_id]\n",
    "    label_shuffle = label[rand_id]\n",
    "    print(context_shuffle,label_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arg_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', default='bert_spc', type=str)\n",
    "    parser.add_argument('--dataset', default='twitter', type=str, help='twitter, restaurant, laptop')\n",
    "    parser.add_argument('--optimizer', default='adam', type=str)\n",
    "    parser.add_argument('--initializer', default='xavier_uniform_', type=str)\n",
    "    parser.add_argument('--learning_rate', default=1e-3, type=float, help='try 5e-5, 2e-5 for BERT, 1e-3 for others')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--l2reg', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epoch', default=40, type=int, help='try larger number for non-BERT models')\n",
    "    parser.add_argument('--batch_size', default=32, type=int, help='try 16, 32, 64 for BERT models')\n",
    "    parser.add_argument('--log_step', default=5, type=int)\n",
    "    parser.add_argument('--embed_dim', default=300, type=int)\n",
    "    parser.add_argument('--hidden_dim', default=300, type=int)\n",
    "    parser.add_argument('--bert_dim', default=768, type=int)\n",
    "    parser.add_argument('--pretrained_bert_name', default='bert-base-uncased', type=str)\n",
    "    parser.add_argument('--max_seq_len', default=80, type=int)\n",
    "    parser.add_argument('--polarities_dim', default=3, type=int)\n",
    "    parser.add_argument('--hops', default=3, type=int)\n",
    "    parser.add_argument('--device', default=None, type=str, help='e.g. cuda:0')\n",
    "    parser.add_argument('--seed', default=None, type=int, help='set seed for reproducibility')\n",
    "    parser.add_argument('--classifier', action=\"store_true\",  help='default True')\n",
    "    parser.add_argument('--add_loss', action=\"store_true\",  help='default True')\n",
    "    parser.add_argument('--tabsa', action=\"store_true\",  help='default True')\n",
    "    parser.add_argument('--tabsa_with_absa', action=\"store_true\",  help='default True') # if true, then use target\n",
    "    parser.add_argument('--classifier_with_absa', action=\"store_true\",  help='default True')\n",
    "    parser.add_argument('--classifier_with_absa_target', action=\"store_true\",  help='default True')\n",
    "    parser.add_argument('--valset_ratio', default=0, type=float, help='set ratio between 0 and 1 for validation support')\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    if opt.seed is not None:\n",
    "        random.seed(opt.seed)\n",
    "        numpy.random.seed(opt.seed)\n",
    "        torch.manual_seed(opt.seed)\n",
    "        torch.cuda.manual_seed(opt.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    model_classes = {\n",
    "        'lstm': LSTM\n",
    "    }\n",
    "    dataset_files = {\n",
    "        'sem15':{\n",
    "            'train':'./datasets/semeval2015/ABSA-15_Restaurants_Train_Final_1197.seg',\n",
    "            'test':'./datasets/semeval2015/ABSA-15_Restaurants_Test_Gold_524.seg.seg',\n",
    "            'classifier':'./datasets/semeval2015/ABSA-15_Restaurants_Train_Final_1197_classifier_tabsa.seg',\n",
    "            'classifier_absa_target':'./datasets/semeval2015/ABSA-15_Restaurants_Train_Final_1197_classifier_tabas_with_target.seg'\n",
    "        },\n",
    "        'sem16':{\n",
    "            'train':'./datasets/semeval2016/ABSA16_Restaurants_Train_SB1_v2_1741.seg',\n",
    "            'test':'./datasets/semeval2016/EN_REST_SB1_TEST.xml.gold_611.seg',\n",
    "            'classifier':'./datasets/semeval2016/ABSA16_Restaurants_Train_SB1_v2_1741_classifier_tabsa.seg',\n",
    "            'classifier_absa_target':'./datasets/semeval2016/ABSA16_Restaurants_Train_SB1_v2_1741_classifier_tabsa_with_target.seg'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    input_colses = {\n",
    "        'lstm': ['text_raw_indices','aspect_indices']\n",
    "    }\n",
    "    \n",
    "    initializers = {\n",
    "        'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
    "        'xavier_normal_': torch.nn.init.xavier_normal,\n",
    "        'orthogonal_': torch.nn.init.orthogonal_\n",
    "    }\n",
    "    optimizers = {\n",
    "        'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
    "        'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
    "        'adam': torch.optim.Adam,  # default lr=0.001\n",
    "        'adamax': torch.optim.Adamax,  # default lr=0.002\n",
    "        'asgd': torch.optim.ASGD,  # default lr=0.01\n",
    "        'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
    "        'sgd': torch.optim.SGD,\n",
    "    }\n",
    "    opt.model_class = model_classes[opt.model_name]\n",
    "    opt.dataset_file = dataset_files[opt.dataset]\n",
    "    opt.inputs_cols = input_colses[opt.model_name]\n",
    "    opt.initializer = initializers[opt.initializer]\n",
    "    opt.optimizer = optimizers[opt.optimizer]\n",
    "    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
    "        if opt.device is None else torch.device(opt.device)\n",
    "\n",
    "    log_file = '{}-{}-{}.log'.format(opt.model_name, opt.dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "    logger.addHandler(logging.FileHandler(log_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_4.0]",
   "language": "python",
   "name": "conda-env-py3_4.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
